# -*- coding: utf-8 -*-
"""
@author: Prabhu <prabhu.appalapuri@gmail.com>
"""

import torch
import torch.nn as nn
import torchvision.transforms as transform
from torch import optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torchvision.datasets as dset
import torchvision.utils as vutils
import numpy as np
from src.generator import Gen
from src.discriminator import Disc

torch.manual_seed(546)

image_size = 64  # using image size as 64x64 with 3 channel i.e 3x64x64
dataset = dset.ImageFolder(root = 'data/celeba', transform = transform.Compose([transform.Resize(image_size),
                                                                            transform.CenterCrop(image_size),
                                                                            transform.ToTensor(),
                                                                            transform.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]))

datasetLoader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)
device = torch.device("cuda:0" if torch.cuda.is_available() else 'cpu')
sample  = iter(datasetLoader).__next__()
# print(vutils.make_grid(sample[0].to(device)[:64], padding= 2, normalize= True).cpu().size())
# print(sample)
plt.figure(figsize=(8,8))
plt.axis('off')
plt.title("Train set images")
plt.imshow(np.transpose(vutils.make_grid(sample[0].to(device)[:64], padding= 2, normalize= True).cpu(),(1,2,0)))

# initialize weights for Generator and Discriminator networks
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') !=-1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)


model_G = Gen(num_GenFeatureMaps = 64, num_OutputChannels = 3, vector_size = 100).to(device)
model_D = Disc(num_Channels = 3, num_DisFeaturesMaps = 64, vector_size = 100).to(device)

model_G.apply(weights_init)
model_D.apply(weights_init)

criterion = nn.BCELoss()

noise = torch.randn(64, 100, 1,1, device=device)
# print(noise)
real_imgLable = 1
fake_imgLabel = 0

optimizerD = optim.Adam(model_D.parameters(), lr= 0.002, betas=(0.5, 0.999))
optimizerG = optim.Adam(model_G.parameters(), lr= 0.002, betas=(0.5, 0.999))


img_list = []
G_loss = []
D_loss = []

for epoch in range(30):
    for i, data in enumerate(datasetLoader):
        """
        Stage 1: The Discriminator network will be trained on real data and also on fake data, which is generated by 
        Generator network. After evaluation of all gradients w.r.t real and fake data, the Discriminator model will be 
        optimized.
        """

        model_D.zero_grad()
        # Stage 1.1 : training from a real input
        real_data = data[0].to(device)
        realSize = real_data.shape[0]
        labels = torch.full((realSize,), real_imgLable, device= device) # torch.Size([128])
        print(labels)
        real_predictions= model_D(real_data)   # torch.Size([128, 1, 1, 1])
        real_predictions.view(-1)  # torch.Size([128])
        real_errD = criterion(real_predictions, labels)
        real_errD.backward()
        D_x = real_predictions.mean().item()

        # Stage 1.2 : training from a fake input
        # 100 means latent vector size, which is the same input value for Generator model
        noise = torch.randn(realSize, 100, 1,1, device= device)
        fake_data= model_G(noise)
        labels.fill_(fake_imgLabel)
        print(labels)
        fake_predictions=model_D(fake_data)  #torch.Size([128, 1, 1, 1])
        fake_predictions.view(-1).cpu()  #torch.Size([128])
        fake_errD = criterion(fake_predictions, labels)
        fake_errD.backward()
        D_G_z1 = fake_predictions.mean().item()
        errD = real_errD + fake_errD
        optimizerD.step()

        """
        stage 2 : which is for updating Generator network after Discriminator network error evaluation D(G(z)) 
        i.e maximization of log(D(G(z))) in order to increase more realistic images from Generator. The more 
        Discriminator network confuses to identify between real and fake then the more error will yield.  
        """
        model_G.zero_grad()
        labels.fill_(real_imgLable)
        predictionsG = model_G(fake_data).view(-1)
        errG = criterion(predictionsG, labels)


        break
    break